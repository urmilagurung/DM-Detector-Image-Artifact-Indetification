{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh2OUqi4oFxt",
        "outputId": "6824db01-1a7b-4f71-c51c-750a5bfaeafd"
      },
      "source": [
        "# Connecting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T257eqFroGax",
        "outputId": "2d833457-8128-479c-fcf7-cf595e2d499c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detection Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import timm\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "# Set paths\n",
        "data_folder = \"Model_10k\"\n",
        "\n",
        "# Function to perform random rotation with seed\n",
        "def random_rotation_with_seed(image, degrees):\n",
        "    # Set the random seed for the rotation\n",
        "    random.seed(random_seed)\n",
        "    return F.rotate(image, degrees)\n",
        "\n",
        "# Preprocessing transforms with data augmentation\n",
        "preprocess_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: random_rotation_with_seed(x, 10)),  # Set the seed for random rotation\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, image_paths, class_labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.class_labels = class_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB mode\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.class_labels[idx]\n",
        "        return image, label\n",
        "\n",
        "# Define a custom binary loss function\n",
        "class BinaryLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryLossFunction, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
        "        return loss\n",
        "\n",
        "def load_data(folder, class_label):\n",
        "    images = [os.path.join(folder, class_label, filename) for filename in os.listdir(os.path.join(folder, class_label)) if filename.endswith(\".png\")]\n",
        "    if \"test\" in folder and not \"Real\" in class_label:\n",
        "        class_labels = [1] * len(images)\n",
        "    else:\n",
        "        class_labels = [1] * len(images) if class_label == \"1_fake\" else [0] * len(images)\n",
        "    return images, class_labels\n",
        "\n",
        "def shuffle_data(image_paths, class_labels):\n",
        "    data = list(zip(image_paths, class_labels))\n",
        "    random.shuffle(data)\n",
        "    image_paths, class_labels = zip(*data)\n",
        "    return image_paths, class_labels\n",
        "\n",
        "def limit_data(image_paths, class_labels, limit):\n",
        "    return image_paths[:limit], class_labels[:limit]\n",
        "\n",
        "# Load images and class labels for training set\n",
        "train_real_images, train_real_labels = load_data(os.path.join(data_folder, \"train\", \"IDDPM\"), \"0_real\") # Change the name of model\n",
        "train_fake_images, train_fake_labels = load_data(os.path.join(data_folder, \"train\", \"IDDPM\"), \"1_fake\") # Change the name of model\n",
        "train_image_paths = train_real_images + train_fake_images\n",
        "train_class_labels = train_real_labels + train_fake_labels\n",
        "train_image_paths, train_class_labels = shuffle_data(train_image_paths, train_class_labels)\n",
        "train_image_paths, train_class_labels = limit_data(train_image_paths, train_class_labels, 10000)\n",
        "\n",
        "# Load images and class labels for validation set\n",
        "val_real_images, val_real_labels = load_data(os.path.join(data_folder, \"val\", \"IDDPM\"), \"0_real\") # Change the name of model\n",
        "val_fake_images, val_fake_labels = load_data(os.path.join(data_folder, \"val\", \"IDDPM\"), \"1_fake\") # Change the name of model\n",
        "val_image_paths = val_real_images + val_fake_images\n",
        "val_class_labels = val_real_labels + val_fake_labels\n",
        "val_image_paths, val_class_labels = shuffle_data(val_image_paths, val_class_labels)\n",
        "val_image_paths, val_class_labels = limit_data(val_image_paths, val_class_labels, 2000)\n",
        "\n",
        "# Load images and class labels for test set\n",
        "test_images, test_labels = load_data(os.path.join(data_folder, \"test\"), \"IDDPM\") # Change the name of model\n",
        "test_image_paths, test_class_labels = shuffle_data(test_images, test_labels)\n",
        "test_image_paths, test_class_labels = limit_data(test_image_paths, test_class_labels, 1000)\n",
        "\n",
        "# Count the occurrences of each class label\n",
        "train_label_counts = Counter(train_class_labels)\n",
        "test_label_counts = Counter(test_class_labels)\n",
        "val_label_counts = Counter(val_class_labels)\n",
        "print(\"Train Label Counts:\", train_label_counts)\n",
        "print(\"Test Label Counts:\", test_label_counts)\n",
        "print(\"Validation Label Counts:\", val_label_counts)\n",
        "\n",
        "# Create train, test, and validation datasets\n",
        "train_dataset = CustomDataset(train_image_paths, train_class_labels, transform=preprocess_transform)\n",
        "test_dataset = CustomDataset(test_image_paths, test_class_labels, transform=preprocess_transform)\n",
        "val_dataset = CustomDataset(val_image_paths, val_class_labels, transform=preprocess_transform)\n",
        "\n",
        "# Create train, test, and validation data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Define your own custom model or use a pretrained model\n",
        "model = timm.create_model('xception', pretrained=True)\n",
        "num_features = model.num_features\n",
        "num_classes = 1\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = BinaryLossFunction()  # Use the custom binary loss function\n",
        "optimizer = Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  # Add weight decay for L2 regularization\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_val_accuracy = 0.0\n",
        "best_model_path = \"best_model_xception_IDDPM.pth\"  # Path to save the best model\n",
        "\n",
        "patience = 5  # Number of epochs to wait for improvement\n",
        "counter = 0  # Counter to track the number of epochs without improvement\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n",
        "\n",
        "    for images, labels in progress_bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().to(device)  # Move labels to device and convert to float\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.view(-1), labels)  # Flatten the output to match the target size\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        sig_outputs = torch.sigmoid(outputs)\n",
        "        predicted = (sig_outputs >= 0.5).float()  # Convert sigmoid outputs to binary predictions\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels.view_as(predicted)).sum().item()  # Compare predictions and labels\n",
        "        progress_bar.set_postfix({'Loss': loss.item(), 'Accuracy': (predicted == labels).sum().item() / labels.size(0)})\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_accuracy = correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluation on the validation set\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct_val = 0\n",
        "    total_val = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.float().to(device)  # Move labels to device and convert to float\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.view(-1), labels)  # Flatten the output to match the target size\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            sig_outputs = torch.sigmoid(outputs)\n",
        "            predicted = (sig_outputs >= 0.5).float()  # Convert sigmoid outputs to binary predictions\n",
        "            total_val += labels.size(0)\n",
        "            correct_val += (predicted == labels.view_as(predicted)).sum().item()  # Compare predictions and labels\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_accuracy = correct_val / total_val\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Learning rate scheduler step\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Check for early stopping based on both loss and accuracy\n",
        "    if val_loss < best_val_loss or val_accuracy > best_val_accuracy:\n",
        "        if best_val_loss > val_loss:\n",
        "            best_val_loss = val_loss\n",
        "        if best_val_accuracy < val_accuracy:\n",
        "            best_val_accuracy = val_accuracy\n",
        "        counter = 0\n",
        "\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Validation loss and accuracy did not improve for {} epochs. Early stopping.\".format(patience))\n",
        "            break\n",
        "\n",
        "# Loading the best model for test evaluation\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "\n",
        "test_loss = 0.0\n",
        "correct_test = 0\n",
        "total_test = 0\n",
        "test_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().to(device)  # Move labels to device and convert to float\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.view(-1), labels)  # Flatten the output to match the target size\n",
        "\n",
        "        test_loss += loss.item()\n",
        "        sig_outputs = torch.sigmoid(outputs)\n",
        "        predicted = (sig_outputs >= 0.5).float()  # Convert sigmoid outputs to binary predictions\n",
        "        total_test += labels.size(0)\n",
        "        correct_test += (predicted == labels.view_as(predicted)).sum().item()  # Compare predictions and labels\n",
        "\n",
        "test_loss /= len(test_loader)\n",
        "test_accuracy = correct_test / total_test\n",
        "# test_losses = [test_loss] * len(val_losses)\n",
        "# test_accuracies = [test_accuracy] * len(val_losses)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Plotting the loss and accuracy curves\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Train')\n",
        "plt.plot(epochs, val_losses, label='Validation')\n",
        "# plt.plot(epochs, test_losses, label='Test')\n",
        "plt.title('Training and Validation and Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, label='Train')\n",
        "plt.plot(epochs, val_accuracies, label='Validation')\n",
        "# plt.plot(epochs, test_accuracies, label='Test')\n",
        "plt.title('Training and Validation and Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAT8JG9xzlqS",
        "outputId": "6dfb6a92-4922-40ae-c445-061a836cd480"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import timm\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random_seed = 42\n",
        "np.random.seed(random_seed)\n",
        "random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "# Set paths\n",
        "data_folder = \"Model_10k\"\n",
        "\n",
        "# Function to perform random rotation with seed\n",
        "def random_rotation_with_seed(image, degrees):\n",
        "    # Set the random seed for the rotation\n",
        "    random.seed(random_seed)\n",
        "    return transforms.functional.rotate(image, degrees)\n",
        "\n",
        "# Preprocessing transforms with data augmentation\n",
        "preprocess_transform = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: random_rotation_with_seed(x, 10)),  # Set the seed for random rotation\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Define custom test dataset class\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, image_paths, class_labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.class_labels = class_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB mode\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = self.class_labels[idx]\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Define a custom binary loss function\n",
        "class BinaryLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BinaryLossFunction, self).__init__()\n",
        "\n",
        "    def forward(self, outputs, labels):\n",
        "        loss = nn.BCEWithLogitsLoss()(outputs, labels)\n",
        "        return loss\n",
        "\n",
        "def load_data(folder, class_label):\n",
        "    images = [os.path.join(folder, class_label, filename) for filename in os.listdir(os.path.join(folder, class_label)) if filename.endswith(\".png\")]\n",
        "    class_labels = [1] * len(images) if class_label == \"1_fake\" else [0] * len(images)\n",
        "    return images, class_labels\n",
        "\n",
        "def shuffle_data(image_paths, class_labels):\n",
        "    data = list(zip(image_paths, class_labels))\n",
        "    random.shuffle(data)\n",
        "    image_paths, class_labels = zip(*data)\n",
        "    return image_paths, class_labels\n",
        "\n",
        "def limit_data(image_paths, class_labels, limit):\n",
        "    return image_paths[:limit], class_labels[:limit]\n",
        "\n",
        "# Load the saved model\n",
        "model = timm.create_model('xception', pretrained=False)\n",
        "num_features = model.num_features\n",
        "num_classes = 1\n",
        "model.fc = nn.Linear(num_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"best_model_xception.pth\")) ## Change the name of weight\n",
        "model.eval()\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Load new test images and labels\n",
        "new_test_real_images, new_test_real_labels = load_data(os.path.join(data_folder, \"test_new\", \"PNDM\"), \"0_real\")  ## change name of architecture\n",
        "new_test_fake_images, new_test_fake_labels = load_data(os.path.join(data_folder, \"test_new\", \"PNDM\"), \"1_fake\")  ## change name of architecture\n",
        "new_test_image_paths = new_test_real_images + new_test_fake_images\n",
        "new_test_class_labels = new_test_real_labels + new_test_fake_labels\n",
        "# new_test_image_paths = new_test_fake_images\n",
        "# new_test_class_labels = new_test_fake_labels\n",
        "new_test_image_paths, new_test_class_labels = shuffle_data(new_test_image_paths, new_test_class_labels)\n",
        "# new_test_image_paths, new_test_class_labels = limit_data(new_test_image_paths, new_test_class_labels, 2000)\n",
        "\n",
        "test_label_counts = Counter(new_test_class_labels)\n",
        "print(\"Test Label Counts:\", test_label_counts)\n",
        "\n",
        "\n",
        "# Create a custom test dataset for new test images\n",
        "new_test_dataset = CustomTestDataset(new_test_image_paths, new_test_class_labels, transform=preprocess_transform)\n",
        "\n",
        "# Create a data loader for new test images\n",
        "new_test_loader = DataLoader(new_test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Create lists to store the predicted probabilities and true labels for real and fake images\n",
        "all_predicted_probabilities = []\n",
        "all_true_labels = []\n",
        "\n",
        "# Evaluation on new test set\n",
        "with torch.no_grad():\n",
        "    for images, labels in tqdm(new_test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.float().to(device)  # Move labels to device and convert to float\n",
        "\n",
        "        outputs = model(images)\n",
        "        sig_outputs = torch.sigmoid(outputs)\n",
        "\n",
        "        all_predicted_probabilities.extend(sig_outputs.cpu().numpy())\n",
        "        all_true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "all_predicted_probabilities = np.array(all_predicted_probabilities)\n",
        "\n",
        "# Separate real and fake images based on class labels\n",
        "real_mask = np.array(new_test_class_labels) == 0\n",
        "fake_mask = np.array(new_test_class_labels) == 1\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, average_precision_score\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, average_precision_score\n",
        "\n",
        "# Assuming the variables `all_true_labels` and `all_predicted_probabilities` contain the true labels and predicted probabilities for the new test images, respectively.\n",
        "\n",
        "# Convert lists to NumPy arrays\n",
        "y_true = np.array(all_true_labels)\n",
        "y_pred = np.array(all_predicted_probabilities)\n",
        "\n",
        "# Calculate accuracy for real and fake images separately\n",
        "r_acc = accuracy_score(y_true[y_true == 0], y_pred[y_true == 0] > 0.5)\n",
        "f_acc = accuracy_score(y_true[y_true == 1], y_pred[y_true == 1] > 0.5)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "acc = accuracy_score(y_true, y_pred > 0.5)\n",
        "\n",
        "# Calculate average precision score\n",
        "ap = average_precision_score(y_true, y_pred)\n",
        "\n",
        "auroc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "\n",
        "# Calculate the probability of detection at a fixed false alarm rate (Pd@FAR) at FARs of 5% and 1%\n",
        "far_5_percent_threshold = np.percentile(y_pred[y_true == 0], 95)\n",
        "far_1_percent_threshold = np.percentile(y_pred[y_true == 0], 99)\n",
        "\n",
        "pd_at_far_5_percent = np.mean(y_pred[y_true == 1] >= far_5_percent_threshold)\n",
        "pd_at_far_1_percent = np.mean(y_pred[y_true == 1] >= far_1_percent_threshold)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy for Real Images: {r_acc:.4f}\")\n",
        "print(f\"Accuracy for Fake Images: {f_acc:.4f}\")\n",
        "print(f\"Overall Accuracy: {acc:.4f}\")\n",
        "print(f\"Average Precision Score: {ap:.4f}\")\n",
        "print(f\"AUROC: {auroc:.4f}\")\n",
        "print(f\"Pd@FAR (FAR = 5%): {pd_at_far_5_percent:.4f}\")\n",
        "print(f\"Pd@FAR (FAR = 1%): {pd_at_far_1_percent:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
